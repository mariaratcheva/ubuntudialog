
# Dialogue Interface for the Baseline Model
import speechbrain
import torch

class Dialogue(sb.pretrained.Pretrained):
    """A ready-to-use Dialogue model
    The class can be used  to  generate a reply to an input prompt.
    # Example
    # -------
    # >>> tmpdir = "./inference_dialogue"
    # >>> source_dir = "/pretrained_Dialogue/"
    # >>> dialogue_model = Dialogue.from_hparams(source=source_dir, savedir=tmpdir, hparams_file='hyperparams.yaml')
    # >>> dialogue_model.chat("How to install Ubuntu?")
    # """

    HPARAMS_NEEDED = []
    MODULES_NEEDED = ["encoder", "encoder_emb", "decoder", "decoder_emb", "seq_lin", "beam_search"]

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.input_encoder_path = self.hparams.input_encoder_path
        self.output_encoder_path = self.hparams.output_encoder_path
        self.special_labels = {
                "blank_label": self.hparams.blank_index,
                "bos_label": self.hparams.bos_index,
                "eos_label": self.hparams.eos_index,
            }
        self.input_encoder = speechbrain.dataio.encoder.CTCTextEncoder()
        self.input_encoder.load_or_create(
                path=self.input_encoder_path,
                special_labels=self.special_labels,
                sequence_input=True,
            )
        self.output_encoder = speechbrain.dataio.encoder.CTCTextEncoder()
        self.output_encoder.load_or_create(
                path=self.output_encoder_path,
                special_labels=self.special_labels,
                sequence_input=True,
            )
        self.modules=self.hparams.modules

    def chat(self, text):
        """Generates a reply to the input prompt.
        Arguments
        ---------
        input_prompt : str
            String representing an input prompt/question.
        Returns
        -------
        str
            The reply generated by the model.
        """
        with torch.no_grad():

            # unpack tokens 
            text_chars = list(text)
            tokens = torch.LongTensor(self.input_encoder.encode_sequence(text_chars))
            tokens_lens = torch.tensor([len(tokens)])
            
            # get the embeddings
            embeddings = self.hparams.encoder_emb(tokens)

            # run the encoder
            encoder_output, token_lens = self.hparams.encoder(embeddings)
            encoder_output = encoder_output.unsqueeze(0)
            
            # make a prediction
            predicted_tokens, scores = self.hparams.beam_searcher(encoder_output, tokens_lens)
            
            # Decode token terms to words
            predicted_words = [
                "".join(self.output_encoder.decode_ndim(utt_seq)).split(" ")
                for utt_seq in predicted_tokens
            ]

            # join in one string
            predicted_words_str ="".join(predicted_words[0])


        return predicted_words_str

    def forward(self, text):
        "Returns the reply to the input text."
        return self.chat(text)
